{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFY9l23KesGCKSc4tAV5sM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugusx/hadoop-practice/blob/main/Hadoop_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Installing hadoop"
      ],
      "metadata": {
        "id": "dQ60qY9Jgwin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cxIc0fwNpTE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --config java"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Dc9KFKBbp84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/hadoop/core/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "!tar -xzvf hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WCgj79o1nWb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.2.3\"\n",
        "os.environ[\"PATH\"] += \":/content/hadoop-3.2.3/bin:/content/hadoop-3.2.3/sbin\""
      ],
      "metadata": {
        "id": "nhdzlJlkpIi-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop version"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dSuXT2j3pNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  Copy paste keseluruhan text Pembukaan UUD 1945 dari website ini dari Judul sampai paragraf terakhir: https://raw.githubusercontent.com/nivdul/spark-in-practice-scala/refs/heads/master/data/wordcount.txt lalu save dengan nama wordcount.txt di local"
      ],
      "metadata": {
        "id": "ypPoT_fEgjEd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5OS9hZdgEZU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!wget -O wordcount.txt https://raw.githubusercontent.com/nivdul/spark-in-practice-scala/refs/heads/master/data/wordcount.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "import re\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    words = re.findall(r'\\w+', line.lower())\n",
        "    for word in words:\n",
        "        print(f'{word}\\t1')"
      ],
      "metadata": {
        "id": "sXkzcqF-mA2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    word, count = line.split('\\t', 1)\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            print(f'{current_word}\\t{current_count}')\n",
        "        current_word = word\n",
        "        current_count = count\n",
        "\n",
        "if current_word == word:\n",
        "    print(f'{current_word}\\t{current_count}')"
      ],
      "metadata": {
        "id": "Y7nrU7BFmFbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Running Mapper and Reducer in Hadoop Cluster"
      ],
      "metadata": {
        "id": "yap50mVPqrIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "    -input wordcount.txt \\\n",
        "    -output wordcount_output \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BOL7qJc7qy59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save output into .txt"
      ],
      "metadata": {
        "id": "UdRkl4XArAvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop fs -getmerge wordcount_output wordcount_result.txt"
      ],
      "metadata": {
        "id": "q_LUJ4ZHrDG-"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}